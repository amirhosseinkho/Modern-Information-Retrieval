{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین اول</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۱۵ آبان <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "<h1>مقدمه</h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "این تمرین به پیش‌پردازش متن، اصلاح پرسمان، ساخت نمایه، بازیابی boolean و فشرده‌سازی نمایه می‌پردازد.\n",
    "<br>\n",
    "دیتاستی که در اختبار شما قرار گرفته است شامل چکیده مقالات و id آن‌ها می‌باشد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> آماده‌سازی دیتاست</h1>\n",
    "<p>\n",
    "دیتاستی که در اختیار شما قرار گرفته است، دارای سطر‌هایی می‌باشد که دارای مقدار NaN می‌باشد. برای اینکه بتوانید با این دیتاست کار کنید، باید ابتدا این سطر‌ها را حذف کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0\n",
      "     paperId                                           abstract\n",
      "0          1  In the recent years many applications have eme...\n",
      "1          2  Many network-on-chip (NoC) designs focus on ma...\n",
      "2          3  Parallelizing the memory accesses in a nested ...\n",
      "3          4  This work surveys the major methods for model-...\n",
      "4          5  Failure detectors are classical mechanisms whi...\n",
      "..       ...                                                ...\n",
      "97        98  Coarse-grained reconfigurable architectures (C...\n",
      "98        99  Hybrid object storage systems provide opportun...\n",
      "99       100  Architectures that aggressively exploit SIMD o...\n",
      "100      101  The Oak Ridge Leadership Computing Facility (O...\n",
      "101      102  DRAM caches have emerged as an efficient new l...\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# load the dataset and remove rows with missing values\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "newdf = df.dropna()\n",
    "\n",
    "print(df['abstract'].isnull().sum())\n",
    "print(newdf['abstract'].isnull().sum())\n",
    "print(newdf.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> Preprocessing (پیش پردازش)</h1>\n",
    "<p>\n",
    "بسیاری از داده ‌ها دارای مقادیر زیادی اطلاعات اضافه هستند که در پردازش ها به آن نیازی نیست و یا باعث ایجاد خطا میشوند.\n",
    "دراین بخش داده را از دیتابیس مورد نظر خوانده\n",
    "  و سپس پیش پردازش های مورد نیاز را اعمال کنید تا متن پیش پردازش شده را تولید کنید.\n",
    "پس از اتمام پیش پردازش سایر عملیات گفته شده در ادامه را بر\n",
    "روی متن ایجاد شده انجام میدهیم.\n",
    "\n",
    "\n",
    "کلاس\n",
    "\"Preprocessor\"\n",
    "عملیات پیش پردازش را انجام میدهد. نام توابع عمل های مورد نظر نوشته شده است و که با توجه به آن باید کد مخصوص هر یک نوشته شود. تابع\n",
    "\"preprocessor\"\n",
    "تابع اصلی این کلاس است که متن بدون پیش پردازش را گرفته و پردازش های مورد نظر را در آن اعمال میکند و متن مورد نظر را ایجاد میکند.\n",
    "\n",
    "در این بخش میتوانید از کتابخانه های آماده مانند\n",
    "<a href=\"https://www.nltk.org/\">NLTK</a>\n",
    "و\n",
    "<a href=\"https://spacy.io/\">SpaCy</a>\n",
    "استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a variable of stop words.\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # The main function of the class.\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        words = self.word_tokenize(text)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text):\n",
    "        # Normalize text (lower case, stemming, lemmatization, etc.)\n",
    "        stemmer = PorterStemmer()\n",
    "        words = word_tokenize(text)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        # Remove links\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        # Remove punctutaions\n",
    "               return re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        # Tokenize text\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        # Remove stopwords\n",
    "        return [word for word in words if word not in self.stopwords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>ساخت نمایه</h1>\n",
    "<p>\n",
    "شما در حال توسعه یک موتور جستجوی سریع هستید که از نمایه سازی پویا پشتیبانی می کند. موتور جستجو اسناد جدید را در قالب دسته‌هایی کوچک‌تر   \n",
    "(batch) هندل می‌کند. در پایان هر روز، این دسته‌ها با استفاده از استراتژی ادغام لگاریتمی ادغام می شوند. هدف به حداقل رساندن هزینه ادغام است.  \n",
    "مراحلی که باید برای حل این مسئله انجام دهید عبارتند از:\n",
    "<li>توکن‌بندی و نرمال‌سازی متن از اسناد.</li>\n",
    "    <li>ایجاد یک index مرتب‌ شده برای هر دسته از اسناد.</li>\n",
    "    <li>ادغام بهینه چند دسته از indexها با استفاده از یک استراتژی ادغام لگاریتمی.</li>\n",
    "وظیفه شما این است که بخش‌های خالی <strong>(مشخص شده به‌صورت {TODO})</strong> کد را پر کنید تا موتور جستجو عملی شود.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h3> دستورات </h3>\n",
    "<li>متد <code>sort_based_index_construction</code> از <code>DocumentBatch</code>:</li>\n",
    "<p>هر سند را با استفاده از تابع‌هایی که در قسمت قبل نوشتید، عمل preprocessing را روی آن انجام دهید.</p>\n",
    "<p>برای هر توکن، شناسه سند را به فهرست معکوس (inverted index) برای آن توکن اضافه کنید.</p>\n",
    "<li>متد <code>add_batch</code> در <code>FastSearchEngine</code></li>\n",
    "<p>فهرست معکوس برای دسته (batch) را ایجاد کنید.</p>\n",
    "<p>این دسته را به فهرست‌های روزانه اضافه کنید.</p>\n",
    "<li>متد <code>end_of_day_merge</code> از <code>FastSearchEngine:</code></li>\n",
    "<p>استراتژی ادغام لگاریتمی را پیاده‌سازی کنید تا به صورت بهینه فهرست‌های روزانه را با فهرست اصلی ادغام کنید.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'sample1': {0}, 'sample2': {1}, 'sample11': {0}, 'sample12': {1}, 'sample3': {4}, 'sample4': {5}, 'sample13': {4}, 'sample14': {5}, 'sample5': {8}, 'sample6': {9}, 'sample15': {8}, 'sample16': {9}, 'sample7': {12}, 'sample8': {13}, 'sample17': {12}, 'sample18': {13}, 'sample9': {16}, 'sample10': {17}, 'sample19': {16}, 'sample20': {17}})\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "class DocumentBatch:\n",
    "    \n",
    "    def __init__(self, docs: List[str], start_id: int):\n",
    "        self.documents = docs\n",
    "        self.index = defaultdict(set)\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.start_id = start_id\n",
    "\n",
    "    def sort_based_index_construction(self):\n",
    "        for doc_id, doc in enumerate(self.documents, start=self.start_id):\n",
    "            tokens = self.preprocess(doc)\n",
    "            for token in tokens:\n",
    "                self.index[token].add(doc_id)\n",
    "\n",
    "    def preprocess(self, doc: str) -> List[str]:\n",
    "        return self.preprocessor.preprocess(doc)\n",
    "\n",
    "\n",
    "class FastSearchEngine:\n",
    "    def __init__(self):\n",
    "        self.main_index = defaultdict(set)\n",
    "        self.daily_indices = deque()\n",
    "        self.next_doc_id = 0\n",
    "\n",
    "    def add_batch(self, batch: DocumentBatch):\n",
    "        batch.sort_based_index_construction()\n",
    "        self.daily_indices.append(batch.index)\n",
    "        self.next_doc_id += len(batch.documents)\n",
    "\n",
    "    def end_of_day_logarithmic_merge(self):\n",
    "        while len(self.daily_indices) > 1:\n",
    "            index_a = self.daily_indices.popleft()\n",
    "            index_b = self.daily_indices.popleft()\n",
    "            merged_index = self.merge_indices(index_a, index_b)\n",
    "            self.daily_indices.append(merged_index)\n",
    "        self.main_index = self.merge_indices(self.main_index, self.daily_indices.popleft())\n",
    "\n",
    "    def merge_indices(self, index_a: dict, index_b: dict) -> dict:\n",
    "        merged_index = defaultdict(set, index_a)\n",
    "        for token, doc_ids in index_b.items():\n",
    "            merged_index[token].update(doc_ids)\n",
    "        return merged_index\n",
    "\n",
    "    def get_inverted_index(self, token):\n",
    "        return self.main_index[token]\n",
    "\n",
    "\n",
    "search_engine = FastSearchEngine()\n",
    "\n",
    "server_a_docs = [\"sample_1\", \"sample_2\", \"sample_3\", \"sample_4\", \"sample_5\", \"sample_6\", \"sample_7\", \"sample_8\", \"sample_9\", \"sample_10\"]\n",
    "server_b_docs = [\"sample_11\", \"sample_12\", \"sample_13\", \"sample_14\", \"sample_15\", \"sample_16\", \"sample_17\", \"sample_18\", \"sample_19\", \"sample_20\"]\n",
    "\n",
    "for i in range(5):\n",
    "    server_a_batch = DocumentBatch(server_a_docs[i*2:i*2+2], search_engine.next_doc_id)\n",
    "    server_b_batch = DocumentBatch(server_b_docs[i*2:i*2+2], search_engine.next_doc_id)\n",
    "\n",
    "    search_engine.add_batch(server_a_batch)\n",
    "    search_engine.add_batch(server_b_batch)\n",
    "\n",
    "    # At end of day\n",
    "    search_engine.end_of_day_logarithmic_merge()\n",
    "\n",
    "print(search_engine.main_index)\n",
    "print(search_engine.get_inverted_index('friend'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>Spell Correction (اصلاح پرسمان)</h1>\n",
    "<p>\n",
    "در بسیاری از اوقات پرسمان دادە شده توسط کاربر، ممکن است ناقص یا دارای غلط املایی باشد. برای رفع این مشکل در بسیاری از موتورهای جستجو راە حل هایی تدارک دیده شدە است. ابتدا این راە حل ها را شرح دهید و بیان کنید که یک موتور جست و جو بر چه اساسی پرسمان های اصلاح شده را به کاربر نمایش می دهد.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>پاسخ سوال بالا</h2>\n",
    "\n",
    "تشخیص خطاهای املایی: موتور جستجو ابتدا سعی می‌کند تا خطاهای املایی را در پرسمان کاربر تشخیص دهد. این کار معمولاً با استفاده از دیکشنری‌های زبان و الگوریتم‌هایی مانند الگوریتم لونشتاین (برای محاسبه فاصله تایپی) انجام می‌شود.\n",
    "\n",
    "پیشنهاد پرسمان‌های اصلاح شده: پس از تشخیص خطاهای املایی، موتور جستجو پرسمان‌های اصلاح شده را پیشنهاد می‌دهد. این پرسمان‌ها معمولاً بر اساس میزان شباهت به پرسمان اصلی و میزان استفاده در جستجوهای گذشته انتخاب می‌شوند.\n",
    "\n",
    "نمایش نتایج مرتبط با پرسمان اصلاح شده: در نهایت، موتور جستجو نتایج مرتبط با پرسمان اصلاح شده را نمایش می‌دهد. این نتایج می‌توانند شامل صفحات وب، تصاویر، ویدیوها و سایر انواع محتوا باشند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "    در این بخش، ابتدا با استفاده از روش bigram لغات نزدیک به لغات اصلی را پیدا کنید و در آخر با معیار minimum edit distance لغتی جایگزین را برای لغت مورد نظر پیدا کنید.  سپس برای هر پرسمان ورودی کاربر، در صورت اشتباه بودن آن، آن را تصحیح کنید. برای stopword‌ها نیز می‌توانید از لیست موجود که در قسمت‌های قبل ساختید استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import edit_distance\n",
    "#nltk.download('words')\n",
    "def create_bigram_index():\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurrence\n",
    "    \"\"\"\n",
    "    bigram: Dict[str, List[str]] = {}\n",
    "    english_words = words.words()\n",
    "    for word in english_words:\n",
    "        for bg in bigrams(word):\n",
    "            bg = ''.join(bg)\n",
    "            if bg not in bigram:\n",
    "                bigram[bg] = []\n",
    "            bigram[bg].append(word)\n",
    "    return bigram\n",
    "\n",
    "bigram_index = create_bigram_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whit is the most popular progambling languaged\n"
     ]
    }
   ],
   "source": [
    "def spell_correction(query):\n",
    "  \"\"\"\n",
    "    Correct the given query text, if it is misspelled\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    query: str\n",
    "        The query text\n",
    "\n",
    "    Returns\n",
    "    ---------\n",
    "    corrected_query: str\n",
    "        The corrected text\n",
    "    \"\"\"\n",
    "  corrected_query = \"\"\n",
    "  for word in query.split():\n",
    "        if word not in words.words():\n",
    "            candidates = set()\n",
    "            for bg in bigrams(word):\n",
    "                bg = ''.join(bg)\n",
    "                if bg in bigram_index:\n",
    "                    candidates.update(bigram_index[bg])\n",
    "            if candidates:\n",
    "                word = min(candidates, key=lambda candidate : edit_distance(word, candidate))\n",
    "        corrected_query += word + \" \"\n",
    "  return corrected_query.strip()\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Wht is the most populr progarmming lanuage?\"\n",
    "print(spell_correction(user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> Boolean Retrieval </h1>\n",
    "<p>\n",
    " در این قسمت هدف طراحی یک سامانەی بازیابی اطلاعات boolean می‌باشد. \n",
    "\n",
    "برای این کار ابتدا پیش پردازش‌های مورد نیاز را مانند بخش قبل بر روی متون انجام دهید و در مرحله بعد ماتریس doc−term را ایجاد کنید. در نهایت کلاس BooleanRetrievalModel را تکمیل کنید که شامل توابع preprocess_query و find_siⅿiⅼar_docs است که توضیحات هرکدام در قسمت کد موجود است. هدف نهایی این است که هرگاه کوئری به تابع find_siⅿiⅼar_docs از کلاس BooleanRetrievalModel داده شود، شناسه k تا از داک‌هایی که شامل کوئری داده شده هستند برگردانده شوند.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def make_doc_term_matrix(documents):\n",
    "    \"\"\"\n",
    "    Create doc_term_matrix by using documents and unique words.\n",
    "    \"\"\"\n",
    "    preprocessor = Preprocessor()\n",
    "    doc_term_matrix = defaultdict(set)\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        tokens = preprocessor.preprocess(doc)\n",
    "        for token in tokens:\n",
    "            doc_term_matrix[token].add(doc_id)\n",
    "    return doc_term_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BooleanRetrievalModel:\n",
    "    def __init__(self, doc_term_matrix):\n",
    "        \"\"\"    \n",
    "        Set doc_term_matrix and initialize the model.\n",
    "        \"\"\"\n",
    "        self.doc_term_matrix = doc_term_matrix\n",
    "        self.preprocessor = Preprocessor()\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        \"\"\"\n",
    "        Do necessary preprocessing here before using the query to find k similar docs.\n",
    "        Use methods from Preprocess section.\n",
    "        \"\"\"\n",
    "        processed_query = self.preprocessor.preprocess(query)\n",
    "        return processed_query\n",
    "    \n",
    "    def find_similar_docs(self, query, k=20):\n",
    "        \"\"\"\n",
    "        Find k similiar documents.\n",
    "        \"\"\"\n",
    "        processed_query = self.preprocess_query(query)\n",
    "        similar_docs = set()\n",
    "        for token in processed_query:\n",
    "            if token in self.doc_term_matrix:\n",
    "                similar_docs.update(self.doc_term_matrix[token])\n",
    "        # If there are more than k similar docs, return the first k docs.\n",
    "        if len(similar_docs) > k:\n",
    "            similar_docs = list(similar_docs)[:k]\n",
    "        return similar_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "در این قسمت ۳ کوئری مختلف به دلخواه خود بزنید و لیست داکیومنت‌های مرتبط با آن‌ها را برگردانید. برای کوتاه‌تر شدن لیست جواب در هر کوئری می‌توانید از عملگر‌های منطقی مانند AND استفاده کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query: hardware\n",
      "Documents related to the query: [2049, 2052, 6149, 7, 4103, 4105, 10, 2064, 2069, 4121, 27, 2075, 2076, 2080, 4129, 4135, 44, 45, 4141, 2098]\n",
      "Executing query: dynamic\n",
      "Documents related to the query: [2, 2050, 4, 4098, 4099, 6149, 8, 4104, 6150, 11, 4108, 13, 14, 6157, 16, 4119, 4120, 2073, 30, 2079]\n",
      "Executing query: query3\n",
      "Documents related to the query: set()\n"
     ]
    }
   ],
   "source": [
    "# Suppose we have the following documents and queries\n",
    "documents =  newdf['abstract'].tolist()\n",
    "queries = [\"hardware\", \"dynamic\", \"query3\"]\n",
    "\n",
    "# First, we create the doc-term matrix\n",
    "doc_term_matrix = make_doc_term_matrix(documents)\n",
    "\n",
    "# Then, we create the Boolean Retrieval Model\n",
    "brm = BooleanRetrievalModel(doc_term_matrix)\n",
    "\n",
    "# Now, we can execute each query and find the related documents\n",
    "for query in queries:\n",
    "    print(f\"Executing query: {query}\")\n",
    "    similar_docs = brm.find_similar_docs(query)\n",
    "    print(f\"Documents related to the query: {similar_docs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "# ذخیره و فشرده‌سازی نمایه\n",
    "در این بخش، در ابتدا دو الگوریتم فشرده‌سازی gamma code و variable byte را پیاده‌سازی کنید.  \n",
    "سپس نمایه را به سه شکل زیر ذخیره کنید:\n",
    "- نمایه‌ی اصلی بدون فشرده‌سازی\n",
    "- نمایه‌ای که با استفاده از gamma code فشرده شده است.\n",
    "- نمایه‌ای که با استفاده از variable byte فشرده شده است.\n",
    "\n",
    "در ادامه اندازه‌ی هر کدام از سه فایل بالا را با استفاده از یک تابع به دست آورده و چاپ کنید.  \n",
    "همچنین باید تابع‌هایی برای decompress کردن نمایه‌های فشرده‌شده پیاده‌سازی کنید.\n",
    "\n",
    "**نکته‌ی ۱:** تمامی نمایه‌ها را نیز در کوئرا ارسال کنید. اگر حجم‌شان بیش‌تر از محدودیت کوئرا است، آن‌ها را در یک مکان دیگر آپلود کرده و لینک آن را در این فایل قرار دهید.  \n",
    "**نکته‌ی ۲:** توابع زیر صرفاً پیشنهادی هستند و هر گونه تغییر تا زمانی که کاربردهای مورد نظر پیاده شود، آزاد است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def gamma_decode_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        gamma_index = pickle.load(f)\n",
    "    decoded_index = defaultdict(list)\n",
    "    for token, gamma_encoded_doc_ids in gamma_index.items():\n",
    "        decoded_index[token] = [gamma_decode(gamma_encoded_doc_id) for gamma_encoded_doc_id in gamma_encoded_doc_ids]\n",
    "    return decoded_index\n",
    "\n",
    "def variable_byte_decode_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        vb_index = pickle.load(f)\n",
    "    decoded_index = defaultdict(list)\n",
    "    for token, vb_encoded_doc_ids in vb_index.items():\n",
    "        decoded_index[token] = variable_byte_decode(vb_encoded_doc_ids)\n",
    "    return decoded_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def gamma_encode(n):\n",
    "    binary_str = bin(n)[2:]  # convert to binary and remove '0b'\n",
    "    unary_part = '1' * (len(binary_str) - 1) + '0'\n",
    "    binary_part = binary_str[1:]\n",
    "    return unary_part + binary_part\n",
    "\n",
    "def save_index_gamma(index, filename):\n",
    "    gamma_index = defaultdict(list)\n",
    "    for token, doc_ids in index.items():\n",
    "        gamma_index[token] = [int(gamma_encode(doc_id), 2) for doc_id in doc_ids]\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(gamma_index, f)\n",
    "\n",
    "def vb_encode_number(n):\n",
    "    bytes_list = []\n",
    "    while True:\n",
    "        bytes_list.insert(0, n % 128)\n",
    "        if n < 128:\n",
    "            break\n",
    "        n = n // 128\n",
    "    bytes_list[-1] += 128  # mark end of bytes for this number\n",
    "    return bytes_list\n",
    "\n",
    "def vb_encode_list(numbers):\n",
    "    bytes_list = []\n",
    "    for number in numbers:\n",
    "        bytes_list.extend(vb_encode_number(number))\n",
    "    return bytes_list\n",
    "\n",
    "def save_index_variable_encode(index, filename):\n",
    "    vb_index = defaultdict(list)\n",
    "    for token, doc_ids in index.items():\n",
    "        vb_index[token] = vb_encode_list(doc_ids)\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(vb_index, f)\n",
    "\n",
    "\n",
    "def save_index(index, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for key, value in index.items():\n",
    "            f.write(key + '\\n')  # write the string\n",
    "            f.write(str(value) + '\\n')  # write the list of doc ids\n",
    "    # Create a list of documents from the 'abstract' column\n",
    "\n",
    "    save_index_gamma(index, \"gamma\")\n",
    "    save_index_variable_encode(index, \"variable\")\n",
    "    #print(engine.main_index)\n",
    "\n",
    "            \n",
    "def load_index():\n",
    "    filename=\"normal\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    gamma_decode_file(\"gamma\")\n",
    "    variable_byte_decode_file(\"variable\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_size():\n",
    "    print(\"size of normal saving:\",os.path.getsize(\"normal\"))\n",
    "    print(\"size of gammma saving:\",os.path.getsize(\"gamma\"))\n",
    "    print(\"size of vaiable saving:\",os.path.getsize(\"variable\"))\n",
    "\n",
    "    \n",
    "documents = newdf['abstract'].tolist()\n",
    "\n",
    "# Create a DocumentBatch and add it to the FastSearchEngine\n",
    "batch = DocumentBatch(documents,0)\n",
    "engine = FastSearchEngine()\n",
    "engine.add_batch(batch)\n",
    "\n",
    "    # At the end of the day, merge the indices\n",
    "engine.end_of_day_logarithmic_merge()\n",
    "index=engine.main_index\n",
    "\n",
    "#load_index()\n",
    "save_index(index,\"normal\")\n",
    "#get_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR-HW1",
   "language": "python",
   "name": "ir-hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}